{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9545360,"sourceType":"datasetVersion","datasetId":5815285}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install libraries","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.transforms import v2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:03.721350Z","iopub.execute_input":"2024-10-05T10:59:03.721743Z","iopub.status.idle":"2024-10-05T10:59:08.736469Z","shell.execute_reply.started":"2024-10-05T10:59:03.721703Z","shell.execute_reply":"2024-10-05T10:59:08.735409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset loading and preparation","metadata":{}},{"cell_type":"code","source":"original_dir = '/kaggle/input/original-and-retouched-faces-images-dataset/original'\nretouched_dir = '/kaggle/input/original-and-retouched-faces-images-dataset/retouched'\n\nnum_images_to_load = 1000  # Set number of images to use in dataset\nbatch_size = 4","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:08.738534Z","iopub.execute_input":"2024-10-05T10:59:08.739029Z","iopub.status.idle":"2024-10-05T10:59:08.743726Z","shell.execute_reply.started":"2024-10-05T10:59:08.738987Z","shell.execute_reply":"2024-10-05T10:59:08.742705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass TrainingConfig:\n    image_size = 256  # the generated image resolution\n    train_batch_size = batch_size\n    eval_batch_size = batch_size  # how many images to sample during evaluation\n    num_epochs = 50\n    gradient_accumulation_steps = 1\n    learning_rate = 1e-4\n    lr_warmup_steps = 500\n    save_image_epochs = 10\n    save_model_epochs = 30\n    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n    output_dir = \"diffuserRet\"  # the model name locally and on the HF Hub\n\n    push_to_hub = False  # whether to upload the saved model to the HF Hub\n    overwrite_output_dir = False  # overwrite the old model when re-running the notebook\n    seed = 42\n\n\nconfig = TrainingConfig()\nconfig","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:08.744929Z","iopub.execute_input":"2024-10-05T10:59:08.745320Z","iopub.status.idle":"2024-10-05T10:59:08.764749Z","shell.execute_reply.started":"2024-10-05T10:59:08.745276Z","shell.execute_reply":"2024-10-05T10:59:08.763885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PairedImageDataset(Dataset):\n    def __init__(self, original_dir, retouched_dir, transform=None, num_images=None, split='train', val_ratio=0.1, test_ratio=0.1):\n        self.original_dir = original_dir\n        self.retouched_dir = retouched_dir\n        self.transform = transform\n        \n        # Get list of image names and ensure the same images are in both folders\n        self.original_images = sorted(os.listdir(original_dir))\n        self.retouched_images = sorted(os.listdir(retouched_dir))\n        self.original_images = [img for img in self.original_images if img in self.retouched_images]\n        \n        # Dynamically choose number of images to load\n        if num_images is not None:\n            self.original_images = self.original_images[:num_images]\n        \n        # Split the dataset into train, validation, and test sets\n        train_images, test_images = train_test_split(self.original_images, test_size=test_ratio, random_state=42)\n        train_images, val_images = train_test_split(train_images, test_size=val_ratio / (1 - test_ratio), random_state=42)\n\n        if split == 'train':\n            self.image_list = train_images\n        elif split == 'val':\n            self.image_list = val_images\n        elif split == 'test':\n            self.image_list = test_images\n        else:\n            raise ValueError(\"Split must be 'train', 'val', or 'test'\")\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, idx):\n        original_image_path = os.path.join(self.original_dir, self.image_list[idx])\n        retouched_image_path = os.path.join(self.retouched_dir, self.image_list[idx])  # Same name\n\n        original_image = Image.open(original_image_path).convert('RGB')\n        retouched_image = Image.open(retouched_image_path).convert('RGB')\n        \n        if self.transform:\n            original_image = self.transform(original_image)\n            retouched_image = self.transform(retouched_image)\n        \n        return original_image, retouched_image","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:08.766876Z","iopub.execute_input":"2024-10-05T10:59:08.767225Z","iopub.status.idle":"2024-10-05T10:59:08.779860Z","shell.execute_reply.started":"2024-10-05T10:59:08.767176Z","shell.execute_reply":"2024-10-05T10:59:08.778899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = [0.5, 0.5, 0.5]\nstd = [0.5, 0.5, 0.5]\n\ntransform = v2.Compose([\n    v2.Resize((config.image_size, config.image_size)),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean, std),\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:08.780969Z","iopub.execute_input":"2024-10-05T10:59:08.782481Z","iopub.status.idle":"2024-10-05T10:59:08.792511Z","shell.execute_reply.started":"2024-10-05T10:59:08.782435Z","shell.execute_reply":"2024-10-05T10:59:08.791658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = PairedImageDataset(original_dir,\n                                   retouched_dir,\n                                   transform=transform,\n                                   num_images=num_images_to_load,\n                                   split='train')\nval_dataset = PairedImageDataset(original_dir,\n                                 retouched_dir,\n                                 transform=transform,\n                                 num_images=num_images_to_load,\n                                 split='val')\ntest_dataset = PairedImageDataset(original_dir,\n                                  retouched_dir,\n                                  transform=transform,\n                                  num_images=num_images_to_load,\n                                  split='test')","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:08.794129Z","iopub.execute_input":"2024-10-05T10:59:08.794474Z","iopub.status.idle":"2024-10-05T10:59:14.288143Z","shell.execute_reply.started":"2024-10-05T10:59:08.794433Z","shell.execute_reply":"2024-10-05T10:59:14.287168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset,\n                          batch_size=batch_size,\n                          shuffle=True)\nval_loader = DataLoader(val_dataset,\n                        batch_size=batch_size,\n                        shuffle=False)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=batch_size,\n                         shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:14.289414Z","iopub.execute_input":"2024-10-05T10:59:14.289819Z","iopub.status.idle":"2024-10-05T10:59:14.295140Z","shell.execute_reply.started":"2024-10-05T10:59:14.289785Z","shell.execute_reply":"2024-10-05T10:59:14.294161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:14.296423Z","iopub.execute_input":"2024-10-05T10:59:14.296726Z","iopub.status.idle":"2024-10-05T10:59:14.308627Z","shell.execute_reply.started":"2024-10-05T10:59:14.296694Z","shell.execute_reply":"2024-10-05T10:59:14.307609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check that images loaded correctly\n\ndef imshow_batch(orig, retouch):\n    orig = orig / 2 + 0.5\n    orig = orig.numpy()\n\n    batch_size = len(orig)\n    _, axes = plt.subplots(1, batch_size, figsize=(batch_size * 3, 2.5))\n    if batch_size == 1: axes = [axes]\n\n    for idx in range(batch_size):\n        ax = axes[idx]\n        img = np.transpose(orig[idx], (1, 2, 0))\n        ax.imshow(img)\n        ax.set_title(\"original\")\n        ax.axis('off')\n    plt.show()\n    \n    retouch = retouch / 2 + 0.5\n    retouch = retouch.numpy()\n    \n    batch_size = len(retouch)\n    _, axes = plt.subplots(1, batch_size, figsize=(batch_size * 3, 2.5))\n    if batch_size == 1: axes = [axes]\n\n    for idx in range(batch_size):\n        ax = axes[idx]\n        img = np.transpose(retouch[idx], (1, 2, 0))\n        ax.imshow(img)\n        ax.set_title(\"retouched\")\n        ax.axis('off')\n    plt.show()\n\n\nfor images, labels in train_loader:\n    imshow_batch(images, labels)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:14.309981Z","iopub.execute_input":"2024-10-05T10:59:14.311194Z","iopub.status.idle":"2024-10-05T10:59:15.880079Z","shell.execute_reply.started":"2024-10-05T10:59:14.311149Z","shell.execute_reply":"2024-10-05T10:59:15.879152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install diffusers","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:15.883539Z","iopub.execute_input":"2024-10-05T10:59:15.884028Z","iopub.status.idle":"2024-10-05T10:59:30.087023Z","shell.execute_reply.started":"2024-10-05T10:59:15.883993Z","shell.execute_reply":"2024-10-05T10:59:30.086053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:30.088545Z","iopub.execute_input":"2024-10-05T10:59:30.088942Z","iopub.status.idle":"2024-10-05T10:59:30.116592Z","shell.execute_reply.started":"2024-10-05T10:59:30.088897Z","shell.execute_reply":"2024-10-05T10:59:30.115550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install invisible_watermark transformers accelerate safetensors","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:30.117824Z","iopub.execute_input":"2024-10-05T10:59:30.118131Z","iopub.status.idle":"2024-10-05T10:59:41.944203Z","shell.execute_reply.started":"2024-10-05T10:59:30.118078Z","shell.execute_reply":"2024-10-05T10:59:41.943119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nfrom datasets import load_dataset\nfrom diffusers import DDIMScheduler, DDPMPipeline\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-10-05T10:59:41.946128Z","iopub.execute_input":"2024-10-05T10:59:41.946557Z","iopub.status.idle":"2024-10-05T10:59:56.021979Z","shell.execute_reply.started":"2024-10-05T10:59:41.946511Z","shell.execute_reply":"2024-10-05T10:59:56.021128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from diffusers import UNet2DConditionModel, DDPMScheduler\nimport torch\nfrom torch.optim import Adam\n\nimage_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\nimage_pipe.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T11:37:54.779717Z","iopub.execute_input":"2024-10-05T11:37:54.780596Z","iopub.status.idle":"2024-10-05T11:37:55.626375Z","shell.execute_reply.started":"2024-10-05T11:37:54.780551Z","shell.execute_reply":"2024-10-05T11:37:55.625491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"execution":{"iopub.status.busy":"2024-10-05T11:38:08.587675Z","iopub.execute_input":"2024-10-05T11:38:08.588075Z","iopub.status.idle":"2024-10-05T11:38:09.613202Z","shell.execute_reply.started":"2024-10-05T11:38:08.588035Z","shell.execute_reply":"2024-10-05T11:38:09.612098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_pipe.scheduler = scheduler\nimages = image_pipe(num_inference_steps=60).images\nimages[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-05T11:38:14.273054Z","iopub.execute_input":"2024-10-05T11:38:14.273489Z","iopub.status.idle":"2024-10-05T11:38:18.366325Z","shell.execute_reply.started":"2024-10-05T11:38:14.273450Z","shell.execute_reply":"2024-10-05T11:38:18.365398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wandb login","metadata":{"execution":{"iopub.status.busy":"2024-10-05T12:30:43.000939Z","iopub.execute_input":"2024-10-05T12:30:43.001953Z","iopub.status.idle":"2024-10-05T12:31:01.386657Z","shell.execute_reply.started":"2024-10-05T12:30:43.001908Z","shell.execute_reply":"2024-10-05T12:31:01.385583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nmodel_traind_name = 'retDiff'\nnum_epochs = 20  # @param\nlr = 1e-5  # 2param\ngrad_accumulation_steps = 2 # @param\nlog_samples_every = 10\nsave_model_every = 20\n\nwandb.init(project='retDiff', config={\n    \"learning_rate\": lr,\n    \"architecture\": \"diffuser\",\n    \"epochs\": num_epochs,\n    \"grad_accumulation_steps\": 2\n    })\noptimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=lr)\n\nlosses = []\n\n# Calculate adjusted learning rate\nadjusted_lr = lr * grad_accumulation_steps\noptimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=adjusted_lr)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T12:52:15.306539Z","iopub.execute_input":"2024-10-05T12:52:15.306958Z","iopub.status.idle":"2024-10-05T12:52:17.355237Z","shell.execute_reply.started":"2024-10-05T12:52:15.306919Z","shell.execute_reply":"2024-10-05T12:52:17.354417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    for step, batch in tqdm(enumerate(train_dataset), total=len(train_dataset)):\n        clean_images = batch[1].to(device)\n        clean_images = clean_images.unsqueeze(0) if clean_images.ndim == 3 else clean_images\n\n        # Sample noise to add to the images\n        noise = torch.randn(clean_images.shape).to(clean_images.device)\n        bs = clean_images.shape[0]\n\n        # Sample a random timestep for each image\n        timesteps = torch.randint(\n            0,\n            image_pipe.scheduler.num_train_timesteps,\n            (bs,),\n            device=clean_images.device,\n        ).long()\n\n        # Add noise to the clean images according to the noise magnitude at each timestep\n        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)\n\n        # Get the model prediction for the noise\n        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]\n\n        # Compare the prediction with the actual noise:\n        loss = F.mse_loss(noise_pred, noise)\n\n        # Store for later plotting\n        losses.append(loss.item())\n        wandb.log({'loss':loss.item()})\n        \n        # Update the model parameters with the optimizer based on this loss\n        loss.backward()\n        \n        # Gradient accumulation:\n        if (step + 1) % grad_accumulation_steps == 0:\n            optimizer.step()  # Update model parameters\n            optimizer.zero_grad()  # Reset gradients\n    \n        if (step+1)%log_samples_every == 0:\n            x = torch.randn(bs, 3, 256, 256).to(device) # Batch of 8\n            for i, t in enumerate(image_pipe.scheduler.timesteps):\n                model_input = image_pipe.scheduler.scale_model_input(x, t)\n                with torch.no_grad():\n                    noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n                x = image_pipe.scheduler.step(noise_pred, t, x).prev_sample\n            grid = torchvision.utils.make_grid(x, nrow=4)\n            im = grid.permute(1, 2, 0).cpu().clip(-1, 1)*0.5 + 0.5\n            im = Image.fromarray(np.array(im*255).astype(np.uint8))\n            wandb.log({'Sample generations': wandb.Image(im)})\n        if (step+1)%save_model_every == 0:\n            image_pipe.save_pretrained(model_save_name+'_latest')\n    # Calculate average loss for the epoch\n    avg_loss = sum(losses[-len(train_dataset):]) / len(train_dataset)\n    print(f\"Epoch {epoch} average loss: {avg_loss}\")\n    \nimage_pipe.save_pretrained(model_save_name)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T12:53:19.553032Z","iopub.execute_input":"2024-10-05T12:53:19.553845Z","iopub.status.idle":"2024-10-05T15:55:12.875427Z","shell.execute_reply.started":"2024-10-05T12:53:19.553800Z","shell.execute_reply":"2024-10-05T15:55:12.874030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_pipe","metadata":{"execution":{"iopub.status.busy":"2024-10-05T12:12:47.991271Z","iopub.execute_input":"2024-10-05T12:12:47.991585Z","iopub.status.idle":"2024-10-05T12:12:47.997524Z","shell.execute_reply.started":"2024-10-05T12:12:47.991551Z","shell.execute_reply":"2024-10-05T12:12:47.996668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = image_pipe(num_inference_steps=60).images\nimages[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-05T15:59:34.296263Z","iopub.execute_input":"2024-10-05T15:59:34.296662Z","iopub.status.idle":"2024-10-05T15:59:38.383048Z","shell.execute_reply.started":"2024-10-05T15:59:34.296624Z","shell.execute_reply":"2024-10-05T15:59:38.382114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2024-10-05T12:12:47.998693Z","iopub.execute_input":"2024-10-05T12:12:47.998992Z","iopub.status.idle":"2024-10-05T12:12:48.007995Z","shell.execute_reply.started":"2024-10-05T12:12:47.998961Z","shell.execute_reply":"2024-10-05T12:12:48.007153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom diffusers import DDPMScheduler, UNet2DModel\n\n# Load the pre-trained UNet model and DDPM scheduler (diffusion process)\nmodel =  image_pipe.unet # Example model\nscheduler = image_pipe.scheduler\nscheduler.set_timesteps(num_inference_steps=60)\n\n# Load and preprocess the image\ndef preprocess_image(image_path, image_size=256):\n    image = Image.open(image_path).convert(\"RGB\")\n    transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor()\n    ])\n    return transform(image).unsqueeze(0).to(device)\n\n# Display image\ndef show_image(tensor_image, title=\"Image\"):\n    image = tensor_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n    plt.imshow(image)\n    plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n\n# Add noise to an image\ndef add_noise(image, scheduler, noise_level=0.5):\n    noise = torch.randn_like(image)\n    timesteps = torch.full((image.shape[0],), int(scheduler.num_train_timesteps * noise_level), device=image.device)\n    noisy_image = scheduler.add_noise(image, noise, timesteps)\n    return noisy_image, timesteps\n\ndef denoise_image(noisy_image, scheduler, model):\n    model.eval()\n    with torch.no_grad():\n        for t in tqdm(scheduler.timesteps):\n            # Predict noise from the noisy image\n            noise_pred = model(noisy_image, t).sample\n            # Denoise step\n            noisy_image = scheduler.step(noise_pred, t, noisy_image).prev_sample\n    return noisy_image\n\n\n# Load and preprocess an example image\noriginal_image = test_dataset[20][0].to(device)\ntarget_imgage = test_dataset[20][1]\n# Display original image\nshow_image(original_image, title=\"Original Image\")\nshow_image(target_imgage, title=\"Target Image\")\n# Add noise to the image\noriginal_image = original_image.unsqueeze(0) if original_image.ndim == 3 else original_image","metadata":{"execution":{"iopub.status.busy":"2024-10-05T16:03:52.159483Z","iopub.execute_input":"2024-10-05T16:03:52.159872Z","iopub.status.idle":"2024-10-05T16:03:52.789820Z","shell.execute_reply.started":"2024-10-05T16:03:52.159833Z","shell.execute_reply":"2024-10-05T16:03:52.788946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add noise to the original image\nfor i in np.linspace(0.01,0.5,num=10):\n    print(i)\n    scheduler.set_timesteps(num_inference_steps=6)\n    noisy_image, timesteps = add_noise(original_image, scheduler,noise_level=i)\n    show_image(noisy_image, title=\"Noisy Image\")\n\n    # Denoise the noisy image\n    denoised_image = denoise_image(noisy_image, scheduler, model)\n\n    # Display the denoised image\n    show_image(denoised_image, title=\"Denoised Image\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T16:12:32.024819Z","iopub.execute_input":"2024-10-05T16:12:32.025396Z","iopub.status.idle":"2024-10-05T16:12:41.196329Z","shell.execute_reply.started":"2024-10-05T16:12:32.025342Z","shell.execute_reply":"2024-10-05T16:12:41.195182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\noutputs = 'outputs'\nif not os.path.exists(outputs):\n    os.mkdir(outputs)\ndef save_image(tensor_image, file_path):\n    \"\"\"\n    Save a tensor image as a PNG file.\n\n    Parameters:\n        tensor_image (torch.Tensor): The image tensor to save (C, H, W).\n        file_path (str): The path where the image will be saved.\n    \"\"\"\n    # Convert tensor image to PIL Image\n    image = tensor_image.squeeze(0).permute(1, 2, 0).clamp(0, 1).cpu().numpy()  # Convert to numpy array\n    image = (image * 255).astype(np.uint8)  # Convert to uint8 format\n    pil_image = Image.fromarray(image)  # Create a PIL Image from the numpy array\n    \n    # Save the image\n    pil_image.save(file_path)\n    #print(f\"Image saved to {file_path}\")\n\nfor step, (image,_) in tqdm(enumerate(test_dataset), total=len(test_dataset)):\n    scheduler.set_timesteps(num_inference_steps=6)\n    original_image = image.to(device)\n    original_image = original_image.unsqueeze(0) if original_image.ndim == 3 else original_image\n    noisy_image, timesteps = add_noise(original_image, scheduler,noise_level=0.2)\n    denoised_image = denoise_image(noisy_image, scheduler, model)\n    save_image(denoised_image,os.path.join(outputs,f'img{step}.png'))\n    show_image(denoised_image, title=step)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T16:13:58.699999Z","iopub.execute_input":"2024-10-05T16:13:58.700427Z","iopub.status.idle":"2024-10-05T16:15:15.630113Z","shell.execute_reply.started":"2024-10-05T16:13:58.700387Z","shell.execute_reply":"2024-10-05T16:15:15.628915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/outputs","metadata":{"execution":{"iopub.status.busy":"2024-10-05T16:20:12.610372Z","iopub.execute_input":"2024-10-05T16:20:12.611170Z","iopub.status.idle":"2024-10-05T16:20:13.916509Z","shell.execute_reply.started":"2024-10-05T16:20:12.611126Z","shell.execute_reply":"2024-10-05T16:20:13.915512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}